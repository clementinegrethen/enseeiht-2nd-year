{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "nlp-sequence-models",
      "graded_item_id": "xxuVc",
      "launcher_item_id": "X20PE"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Construire et entraîner un perceptron multi-couches - étape par étape\n",
        "\n",
        "Dans ce TP, vous allez mettre en œuvre l'entraînement d'un réseau de neurones (perceptron multi-couches) à l'aide de la librairie **numpy**. Pour cela nous allons procéder par étapes successives. Dans un premier temps nous allons traiter le cas d'un perceptron mono-couche, en commençant par la passe *forward* de prédiction d'une sortie à partir d'une entrée et des paramètres du perceptron, puis en implémentant la passe *backward* de calcul des gradients de la fonction objectif par rapport aux paramètres. A partir de là, nous pourrons tester l'entraînement à l'aide de la descente de gradient stochastique.\n",
        "\n",
        "Une fois ces étapes achevées, nous pourrons nous atteler à la construction d'un perceptron multi-couches, qui consistera pour l'essentiel en la composition de perceptrons mono-couche. \n",
        "\n",
        "Dans ce qui suit, nous adoptons les conventions de notation suivantes : \n",
        "\n",
        "- $(x, y)$ désignent un couple donnée/label de la base d'apprentissage ; $\\hat{y}$ désigne quant à lui la prédiction du modèle sur la donnée $x$.\n",
        "\n",
        "- L'indice $i$ indique la $i^{\\text{ème}}$ dimension d'un vecteur ⇒ $a_i$\n",
        "\n",
        "- L'exposant $(k)$ désigne un objet associé au $k^{\\text{ème}}$ exemple ⇒ $a_i^{(k)}$\n",
        "\n",
        "- L'exposant $[l]$ désigne un objet associé à la $l^{\\text{ème}}$ couche ⇒ $a_i^{(k)[l]}$\n",
        "   \n",
        "Exemple:  \n",
        "- $a_5^{(2)[3]}$ indique donc la $5^{\\text{ème}}$ dimension du vecteur d'activation du $2^{\\text{ème}}$ exemple d'entraînement (2), de la $3^{\\text{ème}}$ couche [3].\n",
        "\n",
        "\n",
        "Commençons par importer tous les modules nécessaires : "
      ],
      "metadata": {
        "id": "5b3pjAUEk2LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "R6LBs_NLla1a"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron mono-couche\n"
      ],
      "metadata": {
        "id": "3JZIXefJlXSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perceptron mono-couche - passe *forward*\n",
        "\n",
        "Un perceptron mono-couche est un modèle liant une couche d'entrée (en vert, qui n'effectue pas d'opération) à une couche de sortie. Les neurones des deux couches sont connectés par des liaisons pondérées (les poids synaptiques) $W_{xy}$, et les neurones de la couche de sortie portent chacun un biais additif $b_y$. Enfin, une fonction d'activation $f$ est appliquée à l'issue de ces opérations pour obtenir la prédiction du réseau $\\hat{y}$. \n",
        "\n",
        "On a donc :\n",
        "\n",
        "$$\\hat{y} = f ( W_{xy} x + b_y )$$ \n",
        "\n",
        "On posera pour la suite :\n",
        "$$ z = W_{xy} x + b_y $$\n",
        "\n",
        "La figure montre une représentation de ces opérations sous forme de réseau de neurones (à gauche), mais aussi sous une forme fonctionnelle (à droite) qui permet de bien visualiser l'ordre des opérations.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1RZeiaKue0GLXJr3HRtKkuP6GD8r6I1_Q\" height=300>\n",
        "<img src=\"https://drive.google.com/uc?id=1dnQ6SSdpEX1GDTgoNTrUwA3xjiP9rTYU\" height=250> \n",
        "\n",
        "\n",
        "Notez que les paramètres du perceptron, que nous allons ajuster par un processus d'optimisation, sont donc les poids synaptiques $W_{xy}$ et les biais $b_y$. Par commodité dans le code, nous considérerons également comme un paramètre le choix de la fonction d'activation.\n",
        "\n",
        "**Remarque importante** : En pratique, on traite souvent les données par *batch*, c'est-à-dire que les prédictions sont faites pour plusieurs données simultanément. Ici pour une taille de *batch* de $m$, cela signifie en fait que :\n",
        " \n",
        "$$ x \\in \\mathbb{R}^{4 \\times m} \\text{  et  } y \\in \\mathbb{R}^{5 \\times m}$$ \n"
      ],
      "metadata": {
        "id": "azdcz3QV_k-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complétez la fonction *dense_layer_forward* qui calcule la prédiction  d'un perceptron mono-couche pour une entrée $x$. \n",
        "\n",
        "**Indication**:\n",
        "```\n",
        "La fonction np.matmul permet de réaliser un produit matriciel.\n",
        "```"
      ],
      "metadata": {
        "id": "RBtX2euQDSCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_layer_forward(x, Wxy, by, activation):\n",
        "    \"\"\"\n",
        "    Réalise une unique étape forward de la couche dense telle que décrite dans la figure précédente\n",
        "\n",
        "    Arguments:\n",
        "    x -- l'entrée, tableau numpy de dimension (n_x, m).\n",
        "    Wxy -- Matrice de poids multipliant l'entrée, tableau numpy de shape (n_y, n_x)\n",
        "    by -- Biais additif ajouté à la sortie, tableau numpy de dimension (n_y, 1)\n",
        "    activation -- Chaîne de caractère désignant la fonction d'activation choisie : 'linear', 'sigmoid' ou 'relu'\n",
        "\n",
        "    Retourne :\n",
        "    y_pred -- prédiction, tableau numpy de dimension (n_y, m)\n",
        "    cache -- tuple des valeurs utiles pour la passe backward (rétropropagation du gradient), contient (x, z)\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    \n",
        "    ### A COMPLETER  \n",
        "    # calcul de z\n",
        "    z =np.matmul(Wxy,x)+by\n",
        "    # calcul de la sortie en appliquant la fonction d'activation\n",
        "    if activation == 'relu':\n",
        "        y_pred=np.maximum(0,z)\n",
        "    elif activation == 'sigmoid':\n",
        "      y_pred = 1 / (1 + np.exp(-z))\n",
        "    elif activation == 'linear':\n",
        "      y_pred = z\n",
        "    else:\n",
        "      print(\"Erreur : la fonction d'activation n'est pas implémentée.\")\n",
        "    \n",
        "    ### FIN\n",
        "\n",
        "    # sauvegarde du cache pour la passe backward\n",
        "    cache = (x, z)\n",
        "    \n",
        "    return y_pred, cache"
      ],
      "metadata": {
        "id": "YGYbWrRfmIwx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exécutez les lignes suivantes pour vérifier la validité de votre code :"
      ],
      "metadata": {
        "id": "1dCFTHOqD_Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "x_tmp = np.random.randn(3,10)\n",
        "Wxy = np.random.randn(2,3)\n",
        "by = np.random.randn(2,1)\n",
        "\n",
        "activation = 'relu'\n",
        "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
        "print(\"y_pred.shape = \\n\", y_pred_tmp.shape)\n",
        "\n",
        "print('----------------------------')\n",
        "\n",
        "print(\"activation relu : y_pred[1] =\\n\", y_pred_tmp[1])\n",
        "\n",
        "print('----------------------------')\n",
        "\n",
        "activation = 'sigmoid'\n",
        "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
        "print(\"activation sigmoid : y_pred[1] =\\n\", y_pred_tmp[1])\n",
        "\n",
        "print('----------------------------')\n",
        "\n",
        "activation = 'linear'\n",
        "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
        "print(\"activation linear : y_pred[1] =\\n\", y_pred_tmp[1])\n"
      ],
      "metadata": {
        "id": "B6wlVU37on1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aad4ac6-8bc5-4c8f-b58c-3d99763ee6bf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred.shape = \n",
            " (2, 10)\n",
            "----------------------------\n",
            "activation relu : y_pred[1] =\n",
            " [0.         2.11983968 0.88583246 1.39272594 0.         2.92664609\n",
            " 0.         1.47890228 0.         0.04725575]\n",
            "----------------------------\n",
            "activation sigmoid : y_pred[1] =\n",
            " [0.10851642 0.89281659 0.70802939 0.80102707 0.21934644 0.94914804\n",
            " 0.24545321 0.81440672 0.48495927 0.51181174]\n",
            "----------------------------\n",
            "activation linear : y_pred[1] =\n",
            " [-2.10598556  2.11983968  0.88583246  1.39272594 -1.26947904  2.92664609\n",
            " -1.12301093  1.47890228 -0.06018107  0.04725575]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Affichage attendu**: \n",
        "```Python\n",
        "y_pred.shape = \n",
        " (2, 10)\n",
        "----------------------------\n",
        "activation relu : y_pred[1] =\n",
        " [0.         2.11983968 0.88583246 1.39272594 0.         2.92664609\n",
        " 0.         1.47890228 0.         0.04725575]\n",
        "----------------------------\n",
        "activation sigmoid : y_pred[1] =\n",
        " [0.10851642 0.89281659 0.70802939 0.80102707 0.21934644 0.94914804\n",
        " 0.24545321 0.81440672 0.48495927 0.51181174]\n",
        "----------------------------\n",
        "activation linear : y_pred[1] =\n",
        " [-2.10598556  2.11983968  0.88583246  1.39272594 -1.26947904  2.92664609\n",
        " -1.12301093  1.47890228 -0.06018107  0.04725575]\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "YYbiDw8TptiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perceptron mono-couche - passe *backward*\n",
        "\n",
        "Dans les librairies d'apprentissage profond actuelles, il suffit d'implémenter la passe *forward*, et la passe *backward* est réalisée automatiquement, avec le calcul des gradients (différentiation automatique) et la mise à jour des paramètres. Il est cependant intéressant de comprendre comment fonctionne la passe *backward*, en l'implémentant sur un exemple simple.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1MC8Nxu6BQnpB7cGLwunIbgx9s1FaGw81\" height=350> \n",
        "\n",
        "Il faut calculer les dérivées de la fonction objectif par rapport aux différents paramètres, pour ensuite mettre à jour ces derniers pendant la descente de gradient. Les équations de calcul des gradients sont données ci-dessous (c'est un bon exercice que de les calculer à la main). \n",
        "\n",
        "\\begin{align}\n",
        "\\displaystyle dx &= \\frac{\\partial J}{\\partial x} &= { W_{xy}}^T \\: \\left( d\\hat{y} * \\frac{\\partial \\hat{y}}{\\partial z} \\right) \\tag{1}  \\\\[8pt]\n",
        "\\displaystyle  {dW_{xy}} &= \\frac{\\partial J}{\\partial W_{xy}} &= \\left( d\\hat{y} * \\frac{\\partial \\hat{y}}{\\partial z} \\right) \\: x^{T}\\tag{2} \\\\[8pt]\n",
        "\\displaystyle db_{y} &= \\frac{\\partial J}{\\partial b_y} &= \\sum_{batch} \\left( d\\hat{y} * \\frac{\\partial \\hat{y}}{\\partial z} \\right) \\tag{3} \\\\[8pt]\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Ici, $*$ indique une multiplication élément par élément tandis que l'absence de symbole indique une multiplication matricielle. Par ailleurs $d\\hat{y}$ désigne $\\frac{\\partial J}{\\partial \\hat{y}}$, $dW_{xy}$ désigne $\\frac{\\partial J}{\\partial W_{xy}}$, $db_y$ désigne $\\frac{\\partial J}{\\partial b_y}$ et $dx$ désigne $\\frac{\\partial J}{\\partial x}$ (ces noms ont été choisis pour être utilisables dans le code).\n",
        "\n",
        "Il vous reste à déterminer, par vous même, le terme $\\frac{\\partial \\hat{y}}{\\partial z}$, qui constitue en fait la dérivée de la fonction d'activation évaluée en $z$. Par exemple, pour la fonction d'activation linéaire (l'identité), la dérivée est égale à 1 pour tout $z$. A vous de déterminer, et d'implémenter, la dérivée des fonctions *sigmoid* et *relu*. **Attention aux dimensions : $\\frac{\\partial \\hat{y}}{\\partial z}$ est de même dimension que $z$ et $\\hat{y}$ !**\n",
        "\n"
      ],
      "metadata": {
        "id": "GypgZ8jBqooR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_layer_backward(dy_hat, Wxy, by, activation, cache):\n",
        "    \"\"\"\n",
        "    Implémente la passe backward de la couche dense.\n",
        "\n",
        "    Arguments :\n",
        "    dy_hat -- Gradient de la fonction objectif par rapport à la sortie ŷ, de dimension (n_y, m)\n",
        "    Wxy -- Matrice de poids multipliant l'entrée, tableau numpy de shape (n_y, n_x)\n",
        "    by -- Biais additif ajouté à la sortie, tableau numpy de dimension (n_y, 1)\n",
        "    cache -- dictionnaire python contenant des variables utiles (issu de dense_layer_forward())\n",
        "\n",
        "    Retourne :\n",
        "    gradients -- dictionnaire python contenant les gradients suivants :\n",
        "                        dx -- Gradient de la fonction objectif par rapport aux entrées, de dimension (n_x, m)\n",
        "                        dby -- Gradient de la fonction objectif par rapport aux biais, de dimension (n_y, 1)\n",
        "                        dWxy -- Gradient de la fonction objectif par rapport aux poids synaptiques Wxy, de dimension (n_y, n_x)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Récupérer les informations du cache\n",
        "    (x, z) = cache\n",
        "    l,c=np.shape(z)\n",
        "     \n",
        "    ### A COMPLETER    \n",
        "    # calcul de la sortie en appliquant l'activation\n",
        "    if activation == 'relu':\n",
        "      dyhat_dz =np.where(z<=0,0,1)\n",
        "      print(dyhat_dz)\n",
        "    elif activation == 'sigmoid':\n",
        "      dyhat_dz =  (1 / (1 + np.exp(-z)))*(1 - ( 1 / (1 + np.exp(-z))))\n",
        "    elif activation == 'linear':\n",
        "      dyhat_dz = np.ones(np.shape(z))\n",
        "    else:\n",
        "      print(\"Erreur : la fonction d'activation n'est pas implémentée.\")\n",
        "\n",
        "    # calculer le gradient de la perte par rapport à x\n",
        "    dx = np.matmul(np.transpose(Wxy),dy_hat*dyhat_dz)\n",
        "\n",
        "    # calculer le gradient de la perte par rapport à Wxy\n",
        "    dWxy = np.matmul((dy_hat*dyhat_dz),np.transpose(x))\n",
        "\n",
        "    # calculer le gradient de la perte par rapport à by \n",
        "    # Attention, dby doit être de dimension (n_y, 1), pensez à positionner l'attribut\n",
        "    # keepdims de la fonction numpy.sum() à True !\n",
        "    dby = np.sum(dy_hat*dyhat_dz,axis=1,keepdims=True)\n",
        "\n",
        "    ### FIN\n",
        "    \n",
        "    # Stocker les gradients dans un dictionnaire\n",
        "    gradients = {\"dx\": dx, \"dby\": dby, \"dWxy\": dWxy}\n",
        "    \n",
        "    return gradients"
      ],
      "metadata": {
        "id": "wEi_y3W_rCMc"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exécutez la cellule suivante pour vérifier la validité de votre code :"
      ],
      "metadata": {
        "id": "qQGZTgx20JVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "x_tmp = np.random.randn(3,10)\n",
        "Wxy = np.random.randn(2,3)\n",
        "by = np.random.randn(2,1)\n",
        "dy_hat = np.random.randn(2, 10)\n",
        "activation = 'relu'\n",
        "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
        "gradients = dense_layer_backward(dy_hat, Wxy, by, activation, cache_tmp)\n",
        "print(\"dimensions des différents gradients :\")\n",
        "print(\"dx : \", gradients['dx'].shape)\n",
        "print(\"dby : \", gradients['dby'].shape)\n",
        "print(\"dWxy : \", gradients['dWxy'].shape)\n",
        "\n",
        "print('----------------------------')\n",
        "\n",
        "print(\"activation relu : gradients =\\n\", gradients)\n",
        "\n",
        "print('----------------------------')\n",
        "\n",
        "activation = 'sigmoid'\n",
        "gradients = dense_layer_backward(dy_hat, Wxy, by, activation, cache_tmp)\n",
        "print(\"activation sigmoid : gradients =\\n\", gradients)\n",
        "\n",
        "print('----------------------------')\n",
        "\n",
        "activation = 'linear'\n",
        "gradients = dense_layer_backward(dy_hat, Wxy, by, activation, cache_tmp)\n",
        "print(\"activation linear : gradients =\\n\", gradients)"
      ],
      "metadata": {
        "id": "gGxKksOd0N2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca7948c-72ce-4c26-e619-7f6178a5f1bf"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 1 0 1 0 0]\n",
            " [0 1 1 1 0 1 0 1 0 1]]\n",
            "dimensions des différents gradients :\n",
            "dx :  (3, 10)\n",
            "dby :  (2, 1)\n",
            "dWxy :  (2, 3)\n",
            "----------------------------\n",
            "activation relu : gradients =\n",
            " {'dx': array([[ 0.        , -0.52166355, -0.25370565,  0.29772356,  0.        ,\n",
            "        -0.87533798,  0.        , -0.05523234,  0.        , -0.78697273],\n",
            "       [ 0.        , -0.4142952 , -0.20148817,  0.23644635,  0.        ,\n",
            "        -0.43699238,  0.        , -0.14103828,  0.        , -0.62499867],\n",
            "       [ 0.        , -0.00781663, -0.00380154,  0.0044611 ,  0.        ,\n",
            "        -1.15858431,  0.        ,  0.43029667,  0.        , -0.01179203]]), 'dby': array([[1.05545895],\n",
            "       [1.73350613]]), 'dWxy': array([[-3.41036427, -1.30232405, -0.56109731],\n",
            "       [-0.03287152, -0.82109488,  0.98388063]])}\n",
            "----------------------------\n",
            "activation sigmoid : gradients =\n",
            " {'dx': array([[-0.12452463, -0.16508708, -0.02939735,  0.18918939,  0.19365898,\n",
            "        -0.17366309,  0.02947078,  0.03090249, -0.20097835, -0.40773826],\n",
            "       [-0.07359731, -0.10570831, -0.02843055,  0.1189895 ,  0.14755739,\n",
            "        -0.09647417,  0.02411729,  0.00119749, -0.15435059, -0.27725739],\n",
            "       [-0.1141027 , -0.11516714,  0.02211421,  0.14152872,  0.03059908,\n",
            "        -0.18648155, -0.00271799,  0.10403474, -0.02635951, -0.21268142]]), 'dby': array([[0.51620418],\n",
            "       [0.3562789 ]]), 'dWxy': array([[-0.19619895, -0.04346631, -0.0522999 ],\n",
            "       [-0.2464412 , -0.23312061, -0.09313104]])}\n",
            "----------------------------\n",
            "activation linear : gradients =\n",
            " {'dx': array([[-1.24957905, -1.03490637, -0.12102053,  0.91166167,  1.48244289,\n",
            "        -0.87533798,  0.14141685, -0.05523234, -0.84116226, -2.23963678],\n",
            "       [-0.7391886 , -0.70870384, -0.12537673,  0.58861627,  1.06334861,\n",
            "        -0.43699238,  0.12006129, -0.14103828, -0.63891076, -1.4582823 ],\n",
            "       [-1.14209251, -0.51772912,  0.12802262,  0.61441549,  0.52789632,\n",
            "        -1.15858431, -0.03226814,  0.43029667, -0.1418173 , -1.45503003]]), 'dby': array([[3.97266086],\n",
            "       [1.34123607]]), 'dWxy': array([[-1.13528086,  0.37477333, -1.77404551],\n",
            "       [-0.92324845, -1.86932585, -0.37669553]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Affichage attendu**: \n",
        "```Python\n",
        "dimensions des différents gradients :\n",
        "dx :  (3, 10)\n",
        "dby :  (2, 1)\n",
        "dWxy :  (2, 3)\n",
        "----------------------------\n",
        "activation relu : gradients =\n",
        " {'dx': array([[ 0.        , -0.52166355, -0.25370565,  0.29772356,  0.        ,\n",
        "        -0.87533798,  0.        , -0.05523234,  0.        , -0.78697273],\n",
        "       [ 0.        , -0.4142952 , -0.20148817,  0.23644635,  0.        ,\n",
        "        -0.43699238,  0.        , -0.14103828,  0.        , -0.62499867],\n",
        "       [ 0.        , -0.00781663, -0.00380154,  0.0044611 ,  0.        ,\n",
        "        -1.15858431,  0.        ,  0.43029667,  0.        , -0.01179203]]), 'dby': array([[1.05545895],\n",
        "       [1.73350613]]), 'dWxy': array([[-3.41036427, -1.30232405, -0.56109731],\n",
        "       [-0.03287152, -0.82109488,  0.98388063]])}\n",
        "----------------------------\n",
        "activation sigmoid : gradients =\n",
        " {'dx': array([[-0.12452463, -0.16508708, -0.02939735,  0.18918939,  0.19365898,\n",
        "        -0.17366309,  0.02947078,  0.03090249, -0.20097835, -0.40773826],\n",
        "       [-0.07359731, -0.10570831, -0.02843055,  0.1189895 ,  0.14755739,\n",
        "        -0.09647417,  0.02411729,  0.00119749, -0.15435059, -0.27725739],\n",
        "       [-0.1141027 , -0.11516714,  0.02211421,  0.14152872,  0.03059908,\n",
        "        -0.18648155, -0.00271799,  0.10403474, -0.02635951, -0.21268142]]), 'dby': array([[0.51620418],\n",
        "       [0.3562789 ]]), 'dWxy': array([[-0.19619895, -0.04346631, -0.0522999 ],\n",
        "       [-0.2464412 , -0.23312061, -0.09313104]])}\n",
        "----------------------------\n",
        "activation linear : gradients =\n",
        " {'dx': array([[-1.24957905, -1.03490637, -0.12102053,  0.91166167,  1.48244289,\n",
        "        -0.87533798,  0.14141685, -0.05523234, -0.84116226, -2.23963678],\n",
        "       [-0.7391886 , -0.70870384, -0.12537673,  0.58861627,  1.06334861,\n",
        "        -0.43699238,  0.12006129, -0.14103828, -0.63891076, -1.4582823 ],\n",
        "       [-1.14209251, -0.51772912,  0.12802262,  0.61441549,  0.52789632,\n",
        "        -1.15858431, -0.03226814,  0.43029667, -0.1418173 , -1.45503003]]), 'dby': array([[3.97266086],\n",
        "       [1.34123607]]), 'dWxy': array([[-1.13528086,  0.37477333, -1.77404551],\n",
        "       [-0.92324845, -1.86932585, -0.37669553]])}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "5-_jk20X0QIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut maintenant créer une classe *DenseLayer*, qui comprend en attribut toutes les informations nécessaires à la description d'une couche dense, c'est-à-dire : \n",
        "\n",
        "\n",
        "*   Le nombre de neurones en entrée de la couche dense (input_size)\n",
        "*   Le nombre de neurones en sortie de la couche dense (output_size)\n",
        "*   La fonction d'activation choisie sur cette couche (activation)\n",
        "*   Les poids synaptiques de la couche dense, stockés dans une matrice de taille (output_size, input_size) (Wxy)\n",
        "*   Les biais de la couche dense, stockés dans un vecteur de taille (output_size, 1) (by)\n",
        "\n",
        "On ajoute également un attribut cache qui permettra de stocker les entrées de la couche dense (x) ainsi que les calculs intermédiaires (z) réalisés lors de la passe *forward*, afin d'être réutilisés pour la basse *backward*.\n",
        "\n",
        "A vous de compléter les 4 jalons suivants : \n",
        "\n",
        "*   **L'initialisation des paramètres** Wxy et by : Wxy doit être positionnée suivant l'initialisation de Glorot, c'est-à-dire que ses valeurs sont échantillonnées selon une loi normale uniforme : \n",
        "$$ W_{xy}^{\\{0\\}} \\sim \\mathcal{U}_{\\left[-\\sqrt{\\frac{6}{n_x + n_y}}, \\sqrt{\\frac{6}{n_x + n_y}}\\right]}$$\n",
        "\n",
        "et by est initialisée par un vecteur de zéros de taille (output_size, 1).\n",
        "*   **La fonction *forward***, qui consiste simplement en un appel de la fonction *dense_layer_forward* implémentée précédemment.\n",
        "*   **La fonction *backward***, qui consiste simplement en un appel de la fonction *dense_layer_backward* implémentée précédemment.\n",
        "*   Et enfin **la fonction *update_parameters*** qui applique la mise à jour de la descente de gradient en fonction d'un taux d'apprentissage (*learning_rate*) et des gradients calculés dans la passe *forward*.\n"
      ],
      "metadata": {
        "id": "E5KeDgyO-ZPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "      self.input_size = input_size\n",
        "      self.output_size = output_size\n",
        "      self.activation = activation\n",
        "      self.cache = None  # Le cache sera mis à jour lors de la passe forward\n",
        "      ### A COMPLETER\n",
        "      # Initialisation des poids synaptiques et des biais\n",
        "      borne=-math.sqrt(6/(input_size+output_size))\n",
        "      self.Wxy =np.random.uniform(borne,-borne) \n",
        "      self.by = np.zeros(output_size,1)\n",
        "\n",
        "    def forward(self, x_batch):\n",
        "\n",
        "      y, cache = dense_layer_forward(x_batch, Wxy, by, activation)\n",
        "      self.cache = cache\n",
        "      return y\n",
        "\n",
        "    def backward(self, dy_hat):\n",
        "      return dense_layer_backward(dy_hat, Wxy, by, activation, cache)\n",
        "\n",
        "    def update_parameters(self, gradients, learning_rate):\n",
        "      self.Wxy =gradients['dWxy']*learning_rate+self.Wxy\n",
        "      self.by  = gradients['dby']*learning_rate+self.by\n",
        "    ### FIN"
      ],
      "metadata": {
        "id": "u2K9dp1IL3yM"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vPZuCLkWNM9D"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fonction de coût : erreur quadratique moyenne"
      ],
      "metadata": {
        "id": "9GlEB8K3Lani"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour entraîner notre modèle, nous devons mettre en place un optimiseur. Nous implémenterons la descente de gradient stochastique avec mini-batch. Il nous faut cependant au préalable implanter la fonction de coût que nous utiliserons pour évaluer la qualité de nos prédictions. \n",
        "\n",
        "Pour le moment, nous allons nous contenter d'une erreur quadratique moyenne, qui associée à une fonction d'activation linéaire (l'identité) permet de résoudre les problèmes de régression. \n",
        "\n",
        "La fonction de coût prend en entrée deux paramètres : la vérité-terrain *y_true* et la prédiction du modèle *y_pred* ($\\hat{y}$). Ces deux matrices sont de dimension $n_y \\times m$ (où $m$ désigne le nombre d'éléments du *batch*, et $n_y$ le nombre de neurones de la couche de sortie). La fonction retourne deux grandeurs : $J_{mb}$ qui correspond à l'erreur quadratique moyenne des prédictions par rapport aux vérités-terrains, et $d\\hat{y}$ au gradient de l'erreur quadratique moyenne par rapport aux prédictions. Autrement dit : \n",
        "$$ d\\hat{y}  = \\frac{\\partial J_{mb}}{\\partial \\hat{y}}$$\n",
        "\n",
        "où $\\hat{y}$ correspond à *y_pred*, et $J_{mb}$ à la fonction objectif calculée sur un mini-batch $mb$ de données.\n",
        "\n",
        "Dans le cas de l'erreur quadratique moyenne, on a :    \n",
        "\n",
        "$$ J_{mb} = \\frac{1}{m  n_y} \\sum_{i=1}^{n_y} \\sum_{j=1}^{m} (y_{ij} - \\hat{y}_{ij})^2 $$\n",
        "\n",
        "et \n",
        "\n",
        "$$ \\frac{\\partial J_{mb}}{\\partial \\hat{y}} = \\frac{-2}{m  n_y} (y - \\hat{y})$$\n",
        "\n"
      ],
      "metadata": {
        "id": "2KMcQzlskdI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### A COMPLETER\n",
        "def mean_square_error(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Erreur quadratique moyenne entre prédiction et vérité-terrain\n",
        "\n",
        "  Arguments :\n",
        "  y_true -- labels à prédire (vérité-terrain), de dimension (n_y, m)\n",
        "  y_pred -- prédictions du modèle, de dimension (n_y, m)\n",
        "  Retourne :\n",
        "  J_mb -- l'erreur quadratique moyenne entre y_true et y_pred, scalaire\n",
        "  dy_hat -- dérivée partielle de la fonction objectif par rapport à y_pred, de dimension (n_y, m)\n",
        "  \"\"\"  \n",
        "  J_mb = ...\n",
        "  dy_hat = ...\n",
        "  return J_mb, dy_hat"
      ],
      "metadata": {
        "id": "FRDUnhJma6jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testez votre implémentation avec ce bloc de code : "
      ],
      "metadata": {
        "id": "eNbVKV5K0hWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "y_true = np.random.randn(10,3)\n",
        "y_pred = np.random.randn(10,3)\n",
        "\n",
        "J_mb, dy_hat = mean_square_error(y_true, y_pred)\n",
        "print(\"J_mb = \", J_mb)\n",
        "print(\"dy_hat = \\n\", dy_hat)\n"
      ],
      "metadata": {
        "id": "Wt-ensXM0jL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Affichage attendu**: \n",
        "```Python\n",
        "J_mb =  2.0281433227730186\n",
        "dy_hat = \n",
        " [[-0.15440041  0.01433353 -0.01060006]\n",
        " [ 0.0151842  -0.10244358  0.15259161]\n",
        " [-0.19080814  0.06637484  0.08938421]\n",
        " [ 0.0660943  -0.1102629   0.07816745]\n",
        " [-0.02831607  0.13843393 -0.07219745]\n",
        " [ 0.03085971  0.02422291  0.1985409 ]\n",
        " [ 0.00519635  0.00229253  0.09338597]\n",
        " [-0.09979824 -0.13627393 -0.05678914]\n",
        " [-0.07398335  0.08469007  0.06412491]\n",
        " [ 0.1244581   0.03689836  0.02365238]]\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "2RcgS5JJ0lcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descente de gradient stochastique"
      ],
      "metadata": {
        "id": "uZRnPbBjQvZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La descente de gradient stochastique prend en entrée les paramètres suivants :  \n",
        "*    *x_train* et *y_train* respectivement les données et labels de l'ensemble d'apprentissage (que l'on suppose de taille $N$).\n",
        "*    *model* une instance du modèle que l'on veut entraîner (qui doit implanter les 3 fonctions vues précédemment *forward*, *backward* et *update_parameters*).\n",
        "*    *loss_function* peut prendre deux valeurs : 'mse' (erreur quadratique moyenne) ou 'bce' (entropie croisée binaire, que nous implémenterons par la suite).\n",
        "*    *learning_rate* le taux d'apprentissage choisi pour la descente de gradient.\n",
        "*    *epochs* le nombre de parcours complets de l'ensemble d'apprentissage que l'on veut réaliser.\n",
        "*    *batch_size* la taille de mini-batch désirée pour la descente de gradient stochastique. \n",
        "\n",
        "L'algorithme à implémenter est rappelé ci-dessous :       \n",
        "```\n",
        "N_batch = floor(N/batch_size)\n",
        "\n",
        "Répéter epochs fois\n",
        "\n",
        "  Pour b de 1 à N_batch Faire\n",
        "\n",
        "    - Sélectionner les données x_train_batch et labels y_train_batch du b-ème mini-batch\n",
        "    - Calculer la prédiction y_pred_batch du modèle pour ce mini-batch\n",
        "    - Calculer la perte batch_loss et le gradient de la perte batch_grad par rapport aux prédictions sur ce mini-batch\n",
        "    - Calculer les gradients de la perte par rapport à chaque paramètre du modèle\n",
        "    - Mettre à jour les paramètres du modèle \n",
        "\n",
        "  Fin Pour\n",
        "\n",
        "Fin Répéter\n",
        "\n",
        "```\n",
        "Deux remarques additionnelles :    \n",
        "1. A chaque *epoch*, les *mini-batches* doivent être différents (les données doivent être réparties dans différents *mini-batches*).\n",
        "2. Il est intéressant de calculer (et d'afficher !) la perte moyennée sur l'ensemble d'apprentissage à chaque *epoch*. Pour cela, on peut accumuler les pertes de chaque *mini-batch* sur une *epoch* et diviser l'ensemble par le nombre de *mini-batches*."
      ],
      "metadata": {
        "id": "w2XnUBj2n-Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(x_train, y_train, model, loss_function, learning_rate=0.03, epochs=10, batch_size=10):\n",
        "  \"\"\"\n",
        "  Implémente la descente de gradient stochastique\n",
        "\n",
        "  Arguments :\n",
        "  x_train -- Les données d'apprentissage, de dimension (N, n_x) ; ATTENTION ces\n",
        "             dimensions sont inversées par rapport aux premiers exercices\n",
        "  y_train -- Les labels d'apprentissage, de dimension (N, n_y)\n",
        "  model -- Le modèle initialisé, à optimiser.\n",
        "  loss_function -- la fonction de coût à utiliser pour l'optimisation, qui pourra\n",
        "                   être 'mse' (erreur quadratique moyenne) ou 'bce' (entropie croisée binaire)\n",
        "  learning_rate -- le taux d'apprentissage pour la descente de gradient\n",
        "  epochs -- le nombre de parcours complets de l'ensemble d'apprentissage\n",
        "  batch_size -- le nombre d'éléments considérés dans chaque mini-batch de données\n",
        "\n",
        "  Retourne :\n",
        "  model -- le modèle obtenu à la fin du processus d'optimisation\n",
        "  \"\"\"  \n",
        "  # Nombre de batches par epoch\n",
        "  nb_batches = math.floor(x_train.shape[0] / batch_size)\n",
        "\n",
        "  # Pour gérer le tirage aléatoire des batches parmi les données d'entraînement... \n",
        "  indices = np.arange(x_train.shape[0])\n",
        "\n",
        "  for e in range(epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "\n",
        "    # Nouvelle permutation des indices pour la prochaine epoch\n",
        "    indices = np.random.permutation(indices)\n",
        "\n",
        "    for b in range(nb_batches):\n",
        "\n",
        "      # Sélection des données du batch courant\n",
        "      x_train_batch = x_train[indices[b*batch_size:(b+1)*batch_size]]\n",
        "      y_train_batch = y_train[indices[b*batch_size:(b+1)*batch_size]]\n",
        "\n",
        "      # A ce stade les données et labels du batch sont de dimension (m, n_x) et \n",
        "      # (m, n_y), il faut les transposer pour que les variables soient de la \n",
        "      # dimension attendue par le code que nous avons écrit précédemment\n",
        "      x_train_batch = np.transpose(x_train_batch)\n",
        "      y_train_batch = np.transpose(y_train_batch)\n",
        "\n",
        "      ### A COMPLETER\n",
        "      # Prédiction du modèle pour le batch courant\n",
        "      y_pred_batch = ...\n",
        "\n",
        "      # Calcul de la fonction objectif et de son gradient sur le batch courant\n",
        "      if loss_function == 'mse':\n",
        "        batch_loss, batch_dy_hat = mean_square_error(...)\n",
        "      elif loss_function == 'bce':\n",
        "        batch_loss, batch_dy_hat = binary_cross_entropy(...)\n",
        "\n",
        "      running_loss += batch_loss \n",
        "\n",
        "      # Calcul du gradient de la perte par rapport aux paramètres du modèle\n",
        "      param_updates = ...\n",
        "\n",
        "      # Mise à jour des paramètres du modèle\n",
        "      model.update_parameters(...)\n",
        "      ### FIN\n",
        "\n",
        "    print(f\"Epoch {e:4d} : Loss {running_loss/nb_batches:.4f}\")\n",
        " \n",
        "    \n",
        "  return model\n"
      ],
      "metadata": {
        "id": "lk3lypUOLXbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test sur un problème de régression "
      ],
      "metadata": {
        "id": "9bybDhHivjXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le bloc de code suivant permet de générer et d'afficher un ensemble de données pour un problème de régression linéaire classique. "
      ],
      "metadata": {
        "id": "N7q44eS0vrrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Préparation des données\n",
        "x, y = datasets.make_regression(n_samples=250, n_features=1, n_targets=1, random_state=1, noise=10)\n",
        "y = np.expand_dims(y, 1)\n",
        "\n",
        "plt.plot(x, y, 'b.', label='Ensemble d\\'apprentissage')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nGcIVuALraDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vous de déterminer le nombre de neurones à positionner en entrée et en sortie du perceptron monocouche pour résoudre ce problème. Une fois ceci fait, le code ci-après affiche également la prédiction de votre modèle."
      ],
      "metadata": {
        "id": "q7lfdRFMRFZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### A COMPLETER\n",
        "model = DenseLayer(..., ..., ...)\n",
        "model = SGD(x, y, model, ..., learning_rate=0.1, epochs=10, batch_size=20)\n",
        "### FIN\n",
        "\n",
        "plt.plot(x, y, 'b.', label='Ensemble d\\'apprentissage')\n",
        "\n",
        "x_gen = np.expand_dims(np.linspace(-3, 3, 10), 1)\n",
        "y_gen = np.transpose(model.forward(np.transpose(x_gen)))\n",
        "\n",
        "plt.plot(x_gen, y_gen, 'g-', label='Prédiction du modèle')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GKFJ3c2MmomL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test sur un problème de classification binaire"
      ],
      "metadata": {
        "id": "mA9-6PqLwff4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afin de pouvoir tester notre perceptron mono-couche sur un problème de classification binaire (i.e. effectuer une régression logistique), il est d'abord nécessaire d'implémenter l'entropie croisée binaire.\n",
        "\n",
        "$$ J_{mb} = \\frac{1}{m  n_y} \\sum_{i=1}^{n_y} \\sum_{j=1}^{m} \\left(-y_{ij}  log(\\hat{y}_{ij}) - (1-y_{ij})  log(1-\\hat{y}_{ij})\\right) $$\n",
        "\n",
        "et \n",
        "\n",
        "$$ \\frac{\\partial J_{mb}}{\\partial \\hat{y}} = \\frac{1}{m  n_y} \\left(\\frac{-y}{\\hat{y}} + \\frac{1-y}{1-\\hat{y}}\\right)$$\n",
        "      \n"
      ],
      "metadata": {
        "id": "K9AHAgGBwjro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### A COMPLETER\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "  J_mb = ...\n",
        "  dy_hat = ...\n",
        "\n",
        "  return J_mb, dy_hat"
      ],
      "metadata": {
        "id": "_xCXP-pQb2oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le bloc de code suivant permet de générer et d'afficher un ensemble de données pour un problème de classification binaire classique. "
      ],
      "metadata": {
        "id": "0L3pPIpfSVU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "x, y = datasets.make_blobs(n_samples=250, n_features=2, centers=2, center_box=(- 3, 3), random_state=1)\n",
        "y = np.expand_dims(y, 1)\n",
        "\n",
        "plt.plot(x[y[:,0]==0,0], x[y[:,0]==0,1], 'b.')\n",
        "plt.plot(x[y[:,0]==1,0], x[y[:,0]==1,1], 'r.')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4AxQRaegdntx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A nouveau, vous devez déterminer le nombre de neurones à positionner en entrée et en sortie du perceptron monocouche pour résoudre ce problème. Une fois ceci fait, le code ci-après affiche également la prédiction de votre modèle."
      ],
      "metadata": {
        "id": "X7o-u0kcSk_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### A COMPLETER\n",
        "model = DenseLayer(..., ..., ...)\n",
        "model = SGD(x, y, model, ..., learning_rate=0.3, epochs=50, batch_size=20)\n",
        "### FIN\n",
        "\n",
        "plt.plot(x[y[:,0]==0,0], x[y[:,0]==0,1], 'b.')\n",
        "plt.plot(x[y[:,0]==1,0], x[y[:,0]==1,1], 'r.')\n",
        "\n",
        "x1_gen = np.linspace(-6, 2, 10)\n",
        "x2_gen = -model.Wxy[0,0]*x1_gen/model.Wxy[0,1] - model.by[0,0]/model.Wxy[0,1]\n",
        "\n",
        "plt.plot(x1_gen, x2_gen, 'g-')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdyntT9zSrum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test sur un problème de classification binaire plus complexe"
      ],
      "metadata": {
        "id": "Ypq84RCl0bnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testons maintenant un problème de classification plus complexe : "
      ],
      "metadata": {
        "id": "6OPzEofrSrSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = datasets.make_gaussian_quantiles(n_samples=250, n_features=2, n_classes=2, random_state=1)\n",
        "y = np.expand_dims(y, 1)\n",
        "\n",
        "plt.plot(x[y[:,0]==0,0], x[y[:,0]==0,1], 'b.')\n",
        "plt.plot(x[y[:,0]==1,0], x[y[:,0]==1,1], 'r.')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_IQdphRV0hsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code ci-dessous vous permettra d'afficher la frontière de décision établie par votre modèle :"
      ],
      "metadata": {
        "id": "8Ol3eqKGSyC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_decision_boundaries(model, x, y):\n",
        "  dx, dy = 0.1, 0.1\n",
        "  y_grid, x_grid = np.mgrid[slice(-4, 4 + dy, dy),\n",
        "                  slice(-4, 4 + dx, dx)]\n",
        "\n",
        "\n",
        "  x_gen = np.concatenate((np.expand_dims(np.reshape(y_grid, (-1)),1),np.expand_dims(np.reshape(x_grid, (-1)),1)), axis=1)\n",
        "  z_gen = model.forward(np.transpose(x_gen)).reshape(x_grid.shape)\n",
        "\n",
        "  z_min, z_max = 0, 1\n",
        "\n",
        "  c = plt.pcolor(x_grid, y_grid, z_gen, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
        "  plt.colorbar(c)\n",
        "  plt.plot(x[y==0,0], x[y==0,1], 'r.')\n",
        "  plt.plot(x[y==1,0], x[y==1,1], 'b.')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "lN8d7YK76MBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complétez le code ci-dessous :"
      ],
      "metadata": {
        "id": "SRNifc8KS_MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### A COMPLETER\n",
        "model = DenseLayer(..., ..., ...)\n",
        "model = SGD(x, y, model, ..., learning_rate=0.3, epochs=50, batch_size=20)\n",
        "### FIN\n",
        "\n",
        "print_decision_boundaries(model, x, y[:,0])"
      ],
      "metadata": {
        "id": "E9WV-Az70mR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette fois-ci il n'est pas possible de faire résoudre un problème aussi \"complexe\" à notre simple perceptron monocouche. Nous allons pour cela devoir passer au perceptron multi-couches !"
      ],
      "metadata": {
        "id": "J9jMU_YcTAJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "yiGyXLvum0uI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron multi-couches"
      ],
      "metadata": {
        "id": "HIEVrFXkDdMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implémentation du perceptron multi-couches"
      ],
      "metadata": {
        "id": "6ZWNGM7vVlCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir du perceptron mono-couche créé précédemment, nous pouvons maintenant implémenter un perceptron multi-couches, qui est un véritable réseau de neurones dans la mesure où il met en jeu plusieurs couches de neurones successives. **Concrètement, le perceptron multi-couches est une composition de perceptron monocouches**, chacun prenant en entrée l'activation de sortie de la couche précédente. Prenons l'exemple ci-dessous : \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ILboVqVVwy71lqAwM3ZGm6umCQegvmuV\" height=350> \n",
        "\n",
        "\n",
        "Ce perceptron multi-couches est la composition de deux perceptrons monocouches, le premier liant deux neurones d'entrée à deux neurones de sortie, et le second deux neurones d'entrée à un neurone de sortie.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1hyrrsf8ZpqUcy2_T89HbQX7fpmqtbNwa\" height=350> \n",
        "\n",
        "Voici comment nous l'implémenterons : le perceptron multi-couches consiste simplement en une liste de perceptrons monocouches (*DenseLayer*). A l'initialisation, le perceptron multi-couches est une liste vide, dans laquelle il est possible d'ajouter des couches denses (fonction *add_layer()*). \n",
        "\n",
        "```python\n",
        "model = MultiLayerPerceptron()\n",
        "model.add_layer(DenseLayer(2, 2, 'relu'))\n",
        "model.add_layer(DenseLayer(2, 1, 'sigmoid'))\n",
        "```\n",
        "\n",
        "La fonction *forward()* du perceptron multi-couches consiste en le calcul successif de la sortie des couches denses. Chaque couche dense effectue une prédiction sur la sortie de la couche dense précédente.\n",
        "\n",
        "La fonction *backward()* implémente l'algorithme de rétro-propagation du gradient. Les gradients des paramètres de la dernière couche sont calculés en premier, et sont utilisés pour calculer les gradients de la couche précédente, comme illustré sur cette figure.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1KVH0DWbAwT7R6-XmpqmpWob1jqftqC84\" height=350> "
      ],
      "metadata": {
        "id": "1a6VuuWODu8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerPerceptron:\n",
        "    def __init__(self):\n",
        "      # Initialisation de la liste de couches du perceptron multi-couches à la liste vide\n",
        "      self.layers = []\n",
        "\n",
        "    # Fonction permettant d'ajouter la couche passée en paramètre dans la liste de couches\n",
        "    # du perceptron multi-couches\n",
        "    def add_layer(self, layer):\n",
        "      self.layers.append(layer)\n",
        "\n",
        "    # Fonction réalisant la prédiction du perceptron multi-couches :\n",
        "    # Elle consiste en la prédiction successive de chacune des couches de la liste de couches,\n",
        "    # chacune prenant en entrée la prédiction de la couche précédente\n",
        "    def forward(self, x_batch):\n",
        "    \n",
        "      for i in range(len(self.layers)):\n",
        "        ###  A COMPLETER\n",
        "\n",
        "      return ...\n",
        "\n",
        "    # Fonction de calcul des gradients de la fonction objectif par rapport à chaque paramètre \n",
        "    # du perceptron multi-couches\n",
        "    # L'entrée dy_hat correspond au gradient de la fonction objectif par rapport à la prédiction\n",
        "    # finale du perceptron multi-couches (notée dJ/dŷ sur la figure précédente)\n",
        "    # Cette fonction doit implémenter la rétropropagation du gradient : on parcourt la liste des\n",
        "    # couches en sens inverse (fonction reversed) et le gradient de la fonction objectif par rapport \n",
        "    # à l'entrée d'une couche est utilisé pour calculer les gradients de la couche précédente\n",
        "    # \n",
        "    # Cette fonction retourne une liste de dictionnaires de gradients, de même dimension que le nombre\n",
        "    # de couches\n",
        "    def backward(self, dy_hat):\n",
        "      gradients = []\n",
        "      for i in reversed(range(len(self.layers))):\n",
        "        ### A COMPLETER\n",
        "\n",
        "      return gradients\n",
        "\n",
        "    # Fonction de mise à jour des paramètres en fonction des gradients établis dans la \n",
        "    # fonction backward et d'un taux d'apprentissage\n",
        "    def update_parameters(self, gradients, learning_rate):\n",
        "      for i in range(len(self.layers)):\n",
        "        ### A COMPLETER"
      ],
      "metadata": {
        "id": "RNhqq0KXm4Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test sur le problème plus complexe de classification binaire"
      ],
      "metadata": {
        "id": "GyIW025tVcPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez maintenant tester votre perceptron multi-couches sur le problème précédent. Deux couches suffisent pour résoudre le problème !"
      ],
      "metadata": {
        "id": "JEg5-Z7mVEWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = datasets.make_gaussian_quantiles(n_samples=250, n_features=2, n_classes=2, random_state=1)\n",
        "y = np.expand_dims(y, 1)\n",
        "\n",
        "plt.plot(x[y[:,0]==0,0], x[y[:,0]==0,1], 'b.')\n",
        "plt.plot(x[y[:,0]==1,0], x[y[:,0]==1,1], 'r.')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pijGm1ipwrAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiLayerPerceptron()\n",
        "model.add_layer(DenseLayer(2, 10, 'relu'))\n",
        "model.add_layer(DenseLayer(10, 1, 'sigmoid'))\n",
        "\n",
        "model = SGD(x, y, model, 'bce', learning_rate=0.3, epochs=60, batch_size=20)\n",
        "\n",
        "print_decision_boundaries(model, x, y[:,0])"
      ],
      "metadata": {
        "id": "h3He5gXmxQ1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quelques exercices supplémentaires"
      ],
      "metadata": {
        "id": "SMTeraduVplm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evanescence du gradient"
      ],
      "metadata": {
        "id": "46K0mq5bVvT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testez le réseau suivant sur le problème simple de classification binaire évoqué dans la partie précédente :\n",
        "```python\n",
        "model.add_layer(DenseLayer(2, 10, 'sigmoid'))\n",
        "model.add_layer(DenseLayer(10, 10, 'sigmoid'))\n",
        "model.add_layer(DenseLayer(10, 10, 'sigmoid'))\n",
        "model.add_layer(DenseLayer(10, 10, 'sigmoid'))\n",
        "model.add_layer(DenseLayer(10, 1, 'sigmoid'))\n",
        "```\n",
        "\n",
        " \n",
        "\n",
        "1.   Qu'observez-vous ?\n",
        "2.   Comment résoudre ce problème ?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pVBCGX9iVzdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application à un problème de classification d'image\n"
      ],
      "metadata": {
        "id": "YBChCCJREOuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code ci-dessous vous permet de charger l'ensemble de données CIFAR-10 qui regroupe des imagettes de taille $32 \\times 32$ représentant 10 types d'objets différents. \n",
        "\n",
        "Des images de chat et de chien sont extraites de cet ensemble : à vous de mettre en place un perceptron multi-couches de classification binaire pour apprendre à reconnaître un chien d'un chat dans une image."
      ],
      "metadata": {
        "id": "C7efDmj6WNSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Récupération des données\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# La base de données CIFAR contient des images issues de 10 classes :\n",
        "# 0\tairplane\n",
        "# 1\tautomobile\n",
        "# 2\tbird\n",
        "# 3\tcat\n",
        "# 4\tdeer\n",
        "# 5\tdog\n",
        "# 6\tfrog\n",
        "# 7\thorse\n",
        "# 8\tship\n",
        "# 9\ttruck\n",
        "\n",
        "# Préparation des données pour la classification binaire : \n",
        "\n",
        "# Extraction des images des classes de chat et chien\n",
        "indices_train = np.squeeze(y_train)\n",
        "x_cat_train = x_train[indices_train==3,:]\n",
        "x_dog_train = x_train[indices_train==5,:]\n",
        "\n",
        "indices_test = np.squeeze(y_test)\n",
        "x_cat_test = x_test[indices_test==3,:]\n",
        "x_dog_test = x_test[indices_test==5,:]\n",
        "\n",
        "# Création des données d'apprentissage et de test\n",
        "# Les images sont redimensionnées en vecteurs de dimension 3072 (32*32*3)\n",
        "# On assigne 0 à la classe chat et 1 à la classe chien\n",
        "x_train = np.concatenate((np.resize(x_cat_train[0:250],(250, 32*32*3)), np.resize(x_dog_train[0:250],(250, 32*32*3))), axis=0)\n",
        "y_train = np.expand_dims(np.concatenate((np.zeros((250)), np.ones((250))),axis=0),1)\n",
        "\n",
        "x_test = np.concatenate((np.resize(x_cat_test,(1000, 32*32*3)), np.resize(x_dog_test,(1000, 32*32*3))), axis=0)\n",
        "y_test = np.expand_dims(np.concatenate((np.zeros((1000)), np.ones((1000))),axis=0),1)\n",
        "\n",
        "# Normalisation des entrées\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ],
      "metadata": {
        "id": "ZFyeFRYfEN3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A COMPLETER\n",
        "model = MultiLayerPerceptron()\n",
        "model.add_layer(DenseLayer(..., ..., ...))\n",
        "...\n",
        "# A vous de tester le nombre de couches qui vous semble adéquat\n",
        "\n",
        "model = SGD(x_train, y_train, model, ..., learning_rate=0.03, epochs=10, batch_size=10)"
      ],
      "metadata": {
        "id": "VBzhs000JbHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prédiction du modèle sur les données de test\n",
        "y_pred_test = np.transpose(model.forward(np.transpose(x_test)))\n",
        "\n",
        "# Calcul de la précision : un écart inférieur à 0.5 entre prédiction et label\n",
        "#  est considéré comme bonne prédiction\n",
        "prediction_eval = np.where(np.abs(y_pred_test-y_test)<0.5, 1, 0)\n",
        "overall_test_precision = 100*np.sum(prediction_eval)/y_test.shape[0]\n",
        "print(f\"Précision de {overall_test_precision:2.1f} %\")"
      ],
      "metadata": {
        "id": "hPUcXM60L0-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si vous obtenez une précision supérieure à 50%, votre réseau est meilleur qu'une prédiction aléatoire, ce qui est déjà bien ! Notez qu'ici nous avons circonscrit l'ensemble d'apprentissage à 500 échantillons (250 de chaque classe) car les calculs de produit matriciel sont longs. C'est tout l'intérêt de porter les calculs sur GPU ou TPU, des dispositifs matériels spécialement conçus et optimisés pour paralléliser ces calculs."
      ],
      "metadata": {
        "id": "A1jASzh3PSKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilisation de la librairie Keras"
      ],
      "metadata": {
        "id": "YV4WZTfL0KB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'utilisation d'une librairie comme Keras permet d'abstraire toutes les difficultés présentées dans ce TP : voici par exemple comment résoudre grâce à Keras le premier problème de régression linéaire présenté dans ce TP."
      ],
      "metadata": {
        "id": "XFR3jwelW1jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = datasets.make_regression(n_samples=250, n_features=1, n_targets=1, random_state=1, noise=10)\n",
        "\n",
        "plt.plot(x, y, 'b.', label='Ensemble d\\'apprentissage')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ew3_k9uK0P9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='linear', input_dim=1)) # input_dim indique la dimension de la couche d'entrée, ici 1\n",
        "\n",
        "model.summary() # affiche un résumé du modèle"
      ],
      "metadata": {
        "id": "jBQYiUU-XX9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "sgd = optimizers.SGD(learning_rate=0.1) # On choisit la descente de gradient stochastique, avec un taux d'apprentssage de 0.1\n",
        "\n",
        "# On définit ici, pour le modèle introduit plus tôt, l'optimiseur choisi, la fonction de perte (ici\n",
        "# l'erreur quadratique moyenne pour un problème de régression) et les métriques que l'on veut observer pendant\n",
        "# l'entraînement. L'erreur absolue moyenne (MAE) est un indicateur plus simple à interpréter que la MSE.\n",
        "model.compile(optimizer=sgd,\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mae'])\n",
        "\n",
        "# Entraînement du modèle avec des mini-batchs de taille 20, sur 10 epochs. \n",
        "# Le paramètre validation_split signifie qu'on tire aléatoirement une partie des données\n",
        "# (ici 20%) pour servir d'ensemble de validation\n",
        "history = model.fit(x, y, validation_split=0.2, epochs=10, batch_size=20)\n"
      ],
      "metadata": {
        "id": "S0Vqoo26Xfe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x, y, 'b.', label='Ensemble d\\'apprentissage')\n",
        "\n",
        "x_gen = np.expand_dims(np.linspace(-3, 3, 10), 1)\n",
        "y_gen = model.predict(x_gen)\n",
        "\n",
        "plt.plot(x_gen, y_gen, 'g-', label='Prédiction du modèle')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "46LiNDvGYQdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "S'il vous reste du temps, reprenez les différents problèmes définis précédemment et utilisez la librairie Keras pour les résoudre."
      ],
      "metadata": {
        "id": "kHu5v6lUYqTm"
      }
    }
  ]
}